{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to <code>ztxexp</code>","text":"<p>A lightweight, powerful, and developer-friendly Python library designed to streamline and automate the management, execution, and analysis of computational experiments.</p>"},{"location":"#what-is-ztxexp","title":"What is ztxexp?","text":"<p>In machine learning, data science, or any computationally intensive research, we often need to run hundreds or thousands of experiments. These experiments typically involve complex parameter combinations, long runtimes, and a huge number of result files. Managing all of this manually quickly becomes a nightmare.</p> <p>ztxexp was created to solve this exact pain point. It provides an elegant and fluent API that abstracts the entire experimental workflow into three core stages, allowing you to focus on the experiment itself, rather than tedious administrative tasks.</p>"},{"location":"#core-workflow-manage-run-analyze","title":"Core Workflow: Manage -&gt; Run -&gt; Analyze","text":"<p>The key to understanding <code>ztxexp</code> is its core three-stage workflow:</p> <ol> <li> <p>Manage</p> <ul> <li>Use <code>ztxexp.ExpManager</code> to define your parameter space. Whether it's a simple grid search or complex multi-conditional variants, everything can be easily constructed through a fluent, chainable API. You can also inject custom modification and filtering logic to handle tricky parameter dependencies.</li> </ul> </li> <li> <p>Run</p> <ul> <li>Hand the generated list of configurations over to <code>ztxexp.ExpRunner</code>. It intelligently skips experiments that have already completed successfully and utilizes multi-core CPUs for parallel processing, significantly shortening the experiment cycle. All run statuses and logs are properly recorded.</li> </ul> </li> <li> <p>Analyze</p> <ul> <li>After the experiments are finished, <code>ztxexp.ResultAnalyzer</code> can, with a single command, aggregate the results from all successful runs, generating a clean <code>Pandas DataFrame</code>, a summary CSV file, and a ranked pivot table. It also helps you safely clean up result files from failed or no-longer-needed experiments.</li> </ul> </li> </ol>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Ready to boost your experiment efficiency? Start here:</p> Link Description \ud83d\ude80 Installation &amp; Configuration Learn how to quickly install <code>ztxexp</code> and configure your first project. \ud83e\uddd1\u200d\ud83d\udcbb Quick Start Tutorial Follow a complete end-to-end example to experience the core workflow of ztxexp. \ud83d\udcda Full API Reference Dive deep into the detailed usage and parameter descriptions for every class and function. \ud83d\udca1 More Examples Browse multiple example scripts that include advanced usage and tips."},{"location":"api/","title":"API Reference","text":""},{"location":"api/#ztxexp.manager--experiment-configuration-manager","title":"Experiment Configuration Manager","text":"<p>This module provides tools for managing experiment configurations,  including grid search, variants, filtering, modification, and resuming from completed runs.</p> <p>Classes:</p> Name Description <code>ExpManager</code> <p>Main class for managing experiment configurations.</p>"},{"location":"api/#ztxexp.manager.ExpManager","title":"<code>ExpManager</code>","text":"<p>Manages experiment configurations with support for grid search, variants, filtering, modification, and resuming from completed runs.</p> Source code in <code>ztxexp\\manager.py</code> <pre><code>class ExpManager:\n    \"\"\"\n    Manages experiment configurations with support for grid search, variants,\n    filtering, modification, and resuming from completed runs.\n    \"\"\"\n\n    def __init__(self, base_args: argparse.Namespace):\n        \"\"\"Initializes with a base set of arguments.\"\"\"\n        self._base_configs = [copy.deepcopy(base_args)]\n        self._configs = []\n        self._modifiers = []\n        self._filters = []\n        self._is_generated = False\n\n    def add_grid_search(self, param_grid: dict):\n        \"\"\"\n        Expands configurations by a Cartesian product (grid search) of parameters.\n        This should typically be the first method called after initialization.\n\n        Args:\n            param_grid (dict): e.g., {'lr': [0.1, 0.01], 'd_model': [32, 64]}\n        \"\"\"\n        if not param_grid:\n            return self\n\n        keys = list(param_grid.keys())\n        value_combinations = itertools.product(*(param_grid[key] for key in keys))\n\n        new_configs = []\n        for config in self._base_configs:\n            # Create a fresh set of combinations for each base config\n            # This is useful if you have multiple base configurations\n            combinations_clone, value_combinations = itertools.tee(value_combinations)\n            for combination in combinations_clone:\n                new_config = copy.deepcopy(config)\n                for key, value in zip(keys, combination):\n                    setattr(new_config, key, value)\n                new_configs.append(new_config)\n\n        # Grid search results become the new base for further operations\n        self._base_configs = new_configs\n        return self\n\n    def add_variants(self, variant_space: dict):\n        \"\"\"\n        Adds new configurations as independent variations, not a grid search.\n        For each key-value pair, it creates a new set of configs based on the current ones.\n\n        Args:\n            variant_space (dict): e.g., {'task': ['A', 'B'], 'seed': [100, 200]}\n        \"\"\"\n        if not variant_space:\n            return self\n\n        # Start with the current base configurations\n        variant_configs = []\n\n        for key, values in variant_space.items():\n            for value in values:\n                # Create variants from the original base configs\n                for base_config in self._base_configs:\n                    new_config = copy.deepcopy(base_config)\n                    setattr(new_config, key, value)\n                    variant_configs.append(new_config)\n\n        self._base_configs = variant_configs\n        return self\n\n    def add_modifier(self, modifier_func: callable):\n        \"\"\"\n        Adds a function to modify each configuration.\n        The function must accept a config (Namespace) and return a modified config.\n        \"\"\"\n        self._modifiers.append(modifier_func)\n        return self\n\n    def add_filter(self, filter_func: callable):\n        \"\"\"\n        Adds a function to filter configurations.\n        The function must accept a config (Namespace) and return True to keep it.\n        \"\"\"\n        self._filters.append(filter_func)\n        return self\n\n    def shuffle(self):\n        \"\"\"Randomly shuffles the generated configurations.\"\"\"\n        self._generate_if_needed()\n        random.shuffle(self._configs)\n        return self\n\n    def filter_completed(self, results_path: str, ignore_keys: list[str] = None):\n        \"\"\"Filters out experiments that have already been completed.\"\"\"\n        self._generate_if_needed()\n        if ignore_keys is None:\n            ignore_keys = []  # Default keys to ignore\n\n        completed_configs = self._load_completed_configs(results_path)\n        if not completed_configs:\n            print(\"No completed runs found to filter.\")\n            return self\n\n        unrun_configs = []\n        for config in self._configs:\n            is_completed = any(\n                self._are_configs_equal(vars(config), completed, ignore_keys)\n                for completed in completed_configs\n            )\n            if not is_completed:\n                unrun_configs.append(config)\n\n        print(f\"Generated {len(self._configs)} configs. \"\n              f\"Found {len(completed_configs)} completed. \"\n              f\"Remaining {len(unrun_configs)} to run.\")\n        self._configs = unrun_configs\n        return self\n\n    def get_configs(self) -&gt; list[argparse.Namespace]:\n        \"\"\"\n        Applies all modifiers and filters and returns the final configurations.\n        This is the final step in the manager's pipeline.\n        \"\"\"\n        self._generate_if_needed()\n        return self._configs\n\n    def _generate_if_needed(self):\n        \"\"\"Internal method to apply modifiers and filters once.\"\"\"\n        if self._is_generated:\n            return\n\n        current_configs = self._base_configs\n\n        # Apply modifiers\n        if self._modifiers:\n            modified_configs = []\n            for config in current_configs:\n                temp_config = config\n                for modifier in self._modifiers:\n                    temp_config = modifier(temp_config)\n                modified_configs.append(temp_config)\n            current_configs = modified_configs\n\n        # Apply filters\n        if self._filters:\n            filtered_configs = []\n            for config in current_configs:\n                if all(f(config) for f in self._filters):\n                    filtered_configs.append(config)\n            current_configs = filtered_configs\n\n        self._configs = current_configs\n        self._is_generated = True\n\n    def _load_completed_configs(self, results_path: str) -&gt; list[dict]:\n        \"\"\"Loads all 'args.json' from subfolders as completed experiments.\"\"\"\n        folders = utils.get_subdirectories(results_path)\n        completed_configs = []\n        for folder in folders:\n            args_path = folder / 'args.json'\n            if args_path.exists():\n                args_dict = utils.load_json(str(args_path))\n                if args_dict:\n                    completed_configs.append(args_dict)\n        return completed_configs\n\n    def _are_configs_equal(self, config1: dict, config2: dict, ignore_keys: list[str]) -&gt; bool:\n        \"\"\"Compares two configuration dictionaries.\"\"\"\n        keys1 = set(config1.keys()) - set(ignore_keys)\n        keys2 = set(config2.keys()) - set(ignore_keys)\n\n        # Allow comparison even if one config has extra keys (like metrics)\n        common_keys = keys1.intersection(keys2)\n\n        for key in common_keys:\n            # Handle list vs tuple for JSON compatibility\n            c1_val = tuple(config1[key]) if isinstance(config1[key], list) else config1[key]\n            c2_val = tuple(config2[key]) if isinstance(config2[key], list) else config2[key]\n            if c1_val != c2_val:\n                return False\n        return True\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.__init__","title":"<code>__init__(base_args)</code>","text":"<p>Initializes with a base set of arguments.</p> Source code in <code>ztxexp\\manager.py</code> <pre><code>def __init__(self, base_args: argparse.Namespace):\n    \"\"\"Initializes with a base set of arguments.\"\"\"\n    self._base_configs = [copy.deepcopy(base_args)]\n    self._configs = []\n    self._modifiers = []\n    self._filters = []\n    self._is_generated = False\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.add_filter","title":"<code>add_filter(filter_func)</code>","text":"<p>Adds a function to filter configurations. The function must accept a config (Namespace) and return True to keep it.</p> Source code in <code>ztxexp\\manager.py</code> <pre><code>def add_filter(self, filter_func: callable):\n    \"\"\"\n    Adds a function to filter configurations.\n    The function must accept a config (Namespace) and return True to keep it.\n    \"\"\"\n    self._filters.append(filter_func)\n    return self\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.add_grid_search","title":"<code>add_grid_search(param_grid)</code>","text":"<p>Expands configurations by a Cartesian product (grid search) of parameters. This should typically be the first method called after initialization.</p> <p>Parameters:</p> Name Type Description Default <code>param_grid</code> <code>dict</code> <p>e.g., {'lr': [0.1, 0.01], 'd_model': [32, 64]}</p> required Source code in <code>ztxexp\\manager.py</code> <pre><code>def add_grid_search(self, param_grid: dict):\n    \"\"\"\n    Expands configurations by a Cartesian product (grid search) of parameters.\n    This should typically be the first method called after initialization.\n\n    Args:\n        param_grid (dict): e.g., {'lr': [0.1, 0.01], 'd_model': [32, 64]}\n    \"\"\"\n    if not param_grid:\n        return self\n\n    keys = list(param_grid.keys())\n    value_combinations = itertools.product(*(param_grid[key] for key in keys))\n\n    new_configs = []\n    for config in self._base_configs:\n        # Create a fresh set of combinations for each base config\n        # This is useful if you have multiple base configurations\n        combinations_clone, value_combinations = itertools.tee(value_combinations)\n        for combination in combinations_clone:\n            new_config = copy.deepcopy(config)\n            for key, value in zip(keys, combination):\n                setattr(new_config, key, value)\n            new_configs.append(new_config)\n\n    # Grid search results become the new base for further operations\n    self._base_configs = new_configs\n    return self\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.add_modifier","title":"<code>add_modifier(modifier_func)</code>","text":"<p>Adds a function to modify each configuration. The function must accept a config (Namespace) and return a modified config.</p> Source code in <code>ztxexp\\manager.py</code> <pre><code>def add_modifier(self, modifier_func: callable):\n    \"\"\"\n    Adds a function to modify each configuration.\n    The function must accept a config (Namespace) and return a modified config.\n    \"\"\"\n    self._modifiers.append(modifier_func)\n    return self\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.add_variants","title":"<code>add_variants(variant_space)</code>","text":"<p>Adds new configurations as independent variations, not a grid search. For each key-value pair, it creates a new set of configs based on the current ones.</p> <p>Parameters:</p> Name Type Description Default <code>variant_space</code> <code>dict</code> <p>e.g., {'task': ['A', 'B'], 'seed': [100, 200]}</p> required Source code in <code>ztxexp\\manager.py</code> <pre><code>def add_variants(self, variant_space: dict):\n    \"\"\"\n    Adds new configurations as independent variations, not a grid search.\n    For each key-value pair, it creates a new set of configs based on the current ones.\n\n    Args:\n        variant_space (dict): e.g., {'task': ['A', 'B'], 'seed': [100, 200]}\n    \"\"\"\n    if not variant_space:\n        return self\n\n    # Start with the current base configurations\n    variant_configs = []\n\n    for key, values in variant_space.items():\n        for value in values:\n            # Create variants from the original base configs\n            for base_config in self._base_configs:\n                new_config = copy.deepcopy(base_config)\n                setattr(new_config, key, value)\n                variant_configs.append(new_config)\n\n    self._base_configs = variant_configs\n    return self\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.filter_completed","title":"<code>filter_completed(results_path, ignore_keys=None)</code>","text":"<p>Filters out experiments that have already been completed.</p> Source code in <code>ztxexp\\manager.py</code> <pre><code>def filter_completed(self, results_path: str, ignore_keys: list[str] = None):\n    \"\"\"Filters out experiments that have already been completed.\"\"\"\n    self._generate_if_needed()\n    if ignore_keys is None:\n        ignore_keys = []  # Default keys to ignore\n\n    completed_configs = self._load_completed_configs(results_path)\n    if not completed_configs:\n        print(\"No completed runs found to filter.\")\n        return self\n\n    unrun_configs = []\n    for config in self._configs:\n        is_completed = any(\n            self._are_configs_equal(vars(config), completed, ignore_keys)\n            for completed in completed_configs\n        )\n        if not is_completed:\n            unrun_configs.append(config)\n\n    print(f\"Generated {len(self._configs)} configs. \"\n          f\"Found {len(completed_configs)} completed. \"\n          f\"Remaining {len(unrun_configs)} to run.\")\n    self._configs = unrun_configs\n    return self\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.get_configs","title":"<code>get_configs()</code>","text":"<p>Applies all modifiers and filters and returns the final configurations. This is the final step in the manager's pipeline.</p> Source code in <code>ztxexp\\manager.py</code> <pre><code>def get_configs(self) -&gt; list[argparse.Namespace]:\n    \"\"\"\n    Applies all modifiers and filters and returns the final configurations.\n    This is the final step in the manager's pipeline.\n    \"\"\"\n    self._generate_if_needed()\n    return self._configs\n</code></pre>"},{"location":"api/#ztxexp.manager.ExpManager.shuffle","title":"<code>shuffle()</code>","text":"<p>Randomly shuffles the generated configurations.</p> Source code in <code>ztxexp\\manager.py</code> <pre><code>def shuffle(self):\n    \"\"\"Randomly shuffles the generated configurations.\"\"\"\n    self._generate_if_needed()\n    random.shuffle(self._configs)\n    return self\n</code></pre>"},{"location":"api/#ztxexp.runner--experiment-runner","title":"Experiment Runner","text":"<p>This module provides tools for running experiments, including sequential and parallel execution modes.</p> <p>Classes:</p> Name Description <code>ExpRunner</code> <p>Main class for running experiments.</p>"},{"location":"api/#ztxexp.runner.ExpRunner","title":"<code>ExpRunner</code>","text":"<p>Runs a series of experiments based on a list of configurations, with support for multiple execution modes (sequential, parallel).</p> Source code in <code>ztxexp\\runner.py</code> <pre><code>class ExpRunner:\n    \"\"\"\n    Runs a series of experiments based on a list of configurations,\n    with support for multiple execution modes (sequential, parallel).\n    \"\"\"\n\n    def __init__(self, configs: list[argparse.Namespace], exp_function, results_root: str):\n        \"\"\"\n        Args:\n            configs (list): A list of configuration Namespaces.\n            exp_function (callable): The user-defined function for a single\n                                     experiment. It must accept a single argument:\n                                     (args: Namespace). The 'setting_path' will\n                                     be added to the args object.\n            results_root (str): The root directory to save results.\n        \"\"\"\n        self.configs = configs\n        self.exp_function = exp_function\n        self.results_root = Path(results_root)\n        utils.create_dir(str(self.results_root))\n\n    def _run_single_experiment(self, args: argparse.Namespace):\n        \"\"\"\n        A wrapper that executes a single experiment trial.\n        It now adds 'setting_path' to the args object before calling exp_function.\n        \"\"\"\n        uid = uuid.uuid4().hex[:6]\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        setting_name = f\"{timestamp}_{uid}\"\n        setting_path = self.results_root / setting_name\n\n        # --- KEY CHANGE HERE ---\n        # Add setting and path information directly to the args object\n        args.setting = setting_name\n        args.setting_path = setting_path\n\n        thread_intro = f\"--- Exp {args.setting} ---\"\n        print(thread_intro)\n\n        try:\n            # Step 1: Create directory and save the initial configuration\n            utils.create_dir(str(args.setting_path))\n            # We need to convert Path object to string for JSON serialization\n            args_to_save = vars(args).copy()\n            args_to_save['setting_path'] = str(args.setting_path)\n            utils.save_json(args_to_save, args.setting_path / 'args.json')\n\n            # Step 2: Execute the user's core experiment logic with the updated args\n            self.exp_function(args)\n\n            # Step 3: Create a success marker file upon completion\n            (args.setting_path / SUCCESS_MARKER).touch()\n\n            print(f\"--- Exp {args.setting} Finished Successfully (marked with '{SUCCESS_MARKER}') ---\")\n\n        except Exception:\n            error_msg = f\"!!!!!! Experiment {args.setting} Failed !!!!!!\"\n            print(error_msg)\n            traceback.print_exc()\n            with open(args.setting_path / 'error.log', 'w') as f:\n                f.write(error_msg + \"\\n\" + traceback.format_exc())\n\n    def run(self,\n            execution_mode: str = 'sequential',\n            num_workers: int = 4,\n            dynamic_cpu_threshold: int = 80):\n        \"\"\"\n        Executes all experiments using the specified mode.\n\n        Args:\n            execution_mode (str): The mode of execution. Options:\n                - 'sequential': Runs experiments one by one in the main process.\n                - 'process_pool': Uses the standard library's ProcessPoolExecutor.\n                - 'joblib': Uses the joblib library for robust parallel processing.\n                - 'dynamic': (Conceptual) A custom mode to adjust workers based on CPU load.\n            num_workers (int): The number of parallel worker processes.\n            dynamic_cpu_threshold (int): The CPU usage threshold for the 'dynamic' mode.\n        \"\"\"\n        total_exps = len(self.configs)\n        if not total_exps:\n            print(\"No experiment configurations to run.\")\n            return\n\n        print(f\"\\nStarting experiment run with {total_exps} configurations.\")\n        print(f\"Execution Mode: {execution_mode}, Workers: {num_workers if execution_mode != 'sequential' else 1}\\n\")\n\n        if execution_mode == 'sequential':\n            for args in self.configs:\n                self._run_single_experiment(args)\n\n        elif execution_mode == 'process_pool':\n            with ProcessPoolExecutor(max_workers=num_workers) as executor:\n                # The map function is simple and effective.\n                list(executor.map(self._run_single_experiment, self.configs))\n\n        elif execution_mode == 'joblib':\n            # Joblib is often preferred for scientific computing.\n            Parallel(n_jobs=num_workers, prefer='processes')(\n                delayed(self._run_single_experiment)(args) for args in self.configs)\n\n        elif execution_mode == 'dynamic':\n            # This is a placeholder for your custom dynamic logic.\n            # It ensures that new tasks are only submitted if CPU usage is below a threshold.\n            print(\"Running in dynamic mode. NOTE: This is a conceptual implementation.\")\n            logging.basicConfig(level=logging.INFO, filename='./my_results/dynamic.log', filemode='w')\n            self._dynamic_parallel_run(max_workers=num_workers, cpu_threshold=dynamic_cpu_threshold)\n\n        else:\n            raise ValueError(f\"Invalid execution_mode: '{execution_mode}'. \"\n                             \"Choose from 'sequential', 'process_pool', 'joblib', 'dynamic'.\")\n\n        print(\"\\nAll experiment runs have been processed.\")\n\n    def _dynamic_parallel_run(self, max_workers: int, cpu_threshold: int):\n        \"\"\"\n        A conceptual implementation of a dynamic parallel runner.\n        It submits jobs to a process pool but pauses if CPU usage is too high.\n        \"\"\"\n\n        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n            futures = {executor.submit(self._run_single_experiment, args): args for args in self.configs}\n\n            while futures:\n                # Check CPU usage before waiting for the next task\n                cpu_percent = psutil.cpu_percent(interval=1)\n                logging.info(\n                    f\"Current CPU usage: {cpu_percent}%. Active workers: {executor._max_workers - executor._idle_worker_semaphore._value}\")\n\n                if cpu_percent &lt; cpu_threshold:\n                    # Wait for at least one task to complete\n                    done, _ = as_completed(futures, timeout=60)\n                    if not done:\n                        logging.warning(\"No task completed in 60 seconds.\")\n                        continue\n\n                    for future in done:\n                        # Remove the completed future\n                        futures.pop(future)\n                        logging.info(f\"Task for args {future.result()} completed.\")\n                else:\n                    logging.warning(\n                        f\"CPU usage {cpu_percent}% is above threshold {cpu_threshold}%. Pausing job submission.\")\n                    time.sleep(10)  # Wait before checking again\n</code></pre>"},{"location":"api/#ztxexp.runner.ExpRunner.__init__","title":"<code>__init__(configs, exp_function, results_root)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>configs</code> <code>list</code> <p>A list of configuration Namespaces.</p> required <code>exp_function</code> <code>callable</code> <p>The user-defined function for a single                      experiment. It must accept a single argument:                      (args: Namespace). The 'setting_path' will                      be added to the args object.</p> required <code>results_root</code> <code>str</code> <p>The root directory to save results.</p> required Source code in <code>ztxexp\\runner.py</code> <pre><code>def __init__(self, configs: list[argparse.Namespace], exp_function, results_root: str):\n    \"\"\"\n    Args:\n        configs (list): A list of configuration Namespaces.\n        exp_function (callable): The user-defined function for a single\n                                 experiment. It must accept a single argument:\n                                 (args: Namespace). The 'setting_path' will\n                                 be added to the args object.\n        results_root (str): The root directory to save results.\n    \"\"\"\n    self.configs = configs\n    self.exp_function = exp_function\n    self.results_root = Path(results_root)\n    utils.create_dir(str(self.results_root))\n</code></pre>"},{"location":"api/#ztxexp.runner.ExpRunner.run","title":"<code>run(execution_mode='sequential', num_workers=4, dynamic_cpu_threshold=80)</code>","text":"<p>Executes all experiments using the specified mode.</p> <p>Parameters:</p> Name Type Description Default <code>execution_mode</code> <code>str</code> <p>The mode of execution. Options: - 'sequential': Runs experiments one by one in the main process. - 'process_pool': Uses the standard library's ProcessPoolExecutor. - 'joblib': Uses the joblib library for robust parallel processing. - 'dynamic': (Conceptual) A custom mode to adjust workers based on CPU load.</p> <code>'sequential'</code> <code>num_workers</code> <code>int</code> <p>The number of parallel worker processes.</p> <code>4</code> <code>dynamic_cpu_threshold</code> <code>int</code> <p>The CPU usage threshold for the 'dynamic' mode.</p> <code>80</code> Source code in <code>ztxexp\\runner.py</code> <pre><code>def run(self,\n        execution_mode: str = 'sequential',\n        num_workers: int = 4,\n        dynamic_cpu_threshold: int = 80):\n    \"\"\"\n    Executes all experiments using the specified mode.\n\n    Args:\n        execution_mode (str): The mode of execution. Options:\n            - 'sequential': Runs experiments one by one in the main process.\n            - 'process_pool': Uses the standard library's ProcessPoolExecutor.\n            - 'joblib': Uses the joblib library for robust parallel processing.\n            - 'dynamic': (Conceptual) A custom mode to adjust workers based on CPU load.\n        num_workers (int): The number of parallel worker processes.\n        dynamic_cpu_threshold (int): The CPU usage threshold for the 'dynamic' mode.\n    \"\"\"\n    total_exps = len(self.configs)\n    if not total_exps:\n        print(\"No experiment configurations to run.\")\n        return\n\n    print(f\"\\nStarting experiment run with {total_exps} configurations.\")\n    print(f\"Execution Mode: {execution_mode}, Workers: {num_workers if execution_mode != 'sequential' else 1}\\n\")\n\n    if execution_mode == 'sequential':\n        for args in self.configs:\n            self._run_single_experiment(args)\n\n    elif execution_mode == 'process_pool':\n        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n            # The map function is simple and effective.\n            list(executor.map(self._run_single_experiment, self.configs))\n\n    elif execution_mode == 'joblib':\n        # Joblib is often preferred for scientific computing.\n        Parallel(n_jobs=num_workers, prefer='processes')(\n            delayed(self._run_single_experiment)(args) for args in self.configs)\n\n    elif execution_mode == 'dynamic':\n        # This is a placeholder for your custom dynamic logic.\n        # It ensures that new tasks are only submitted if CPU usage is below a threshold.\n        print(\"Running in dynamic mode. NOTE: This is a conceptual implementation.\")\n        logging.basicConfig(level=logging.INFO, filename='./my_results/dynamic.log', filemode='w')\n        self._dynamic_parallel_run(max_workers=num_workers, cpu_threshold=dynamic_cpu_threshold)\n\n    else:\n        raise ValueError(f\"Invalid execution_mode: '{execution_mode}'. \"\n                         \"Choose from 'sequential', 'process_pool', 'joblib', 'dynamic'.\")\n\n    print(\"\\nAll experiment runs have been processed.\")\n</code></pre>"},{"location":"api/#ztxexp.analyzer--experiment-result-analyzer","title":"Experiment Result Analyzer","text":"<p>This module provides tools for analyzing experiment results, including aggregating results into DataFrames, saving to CSV/Excel files, and  cleaning up result directories.</p> <p>Classes:</p> Name Description <code>ResultAnalyzer</code> <p>Main class for analyzing experiment results.</p>"},{"location":"api/#ztxexp.analyzer.ResultAnalyzer","title":"<code>ResultAnalyzer</code>","text":"Source code in <code>ztxexp\\analyzer.py</code> <pre><code>class ResultAnalyzer:\n    def __init__(self, results_path: str):\n        self.results_path = Path(results_path)\n        if not self.results_path.exists():\n            raise FileNotFoundError(f\"Results path does not exist: {self.results_path}\")\n\n    def to_dataframe(self, results_filename: str = 'results.json') -&gt; pd.DataFrame:\n        \"\"\"\n        Aggregates results into a pandas DataFrame.\n        It now only considers folders with a _SUCCESS marker and merges\n        'args.json' with a specified results file.\n\n        Args:\n            results_filename (str): The name of the file containing experiment\n                                    metrics (e.g., 'results.json').\n        \"\"\"\n        folders = utils.get_subdirectories(str(self.results_path))\n        records = []\n\n        print(f\"Analyzing results... Looking for '{SUCCESS_MARKER}' and '{results_filename}'.\")\n\n        for folder in folders:\n            # ONLY consider folders that have the success marker\n            if not (folder / SUCCESS_MARKER).exists():\n                continue\n\n            args_path = folder / 'args.json'\n            if args_path.exists():\n                record = utils.load_json(str(args_path))\n                if not record:\n                    continue\n\n                # Load results file and merge it into the record\n                results_path = folder / results_filename\n                if results_path.exists():\n                    results_data = utils.load_json(str(results_path))\n                    if results_data:\n                        record.update(results_data)\n\n                record['setting_path'] = str(folder.resolve())\n                record['creation_time'] = utils.get_file_creation_time(args_path)\n                records.append(record)\n\n        if not records:\n            print(\"No successfully completed experiments found.\")\n            return pd.DataFrame()\n\n        return pd.DataFrame.from_records(records)\n\n    def to_csv(self, output_path: str, sort_by: list[str] = None):\n        \"\"\"Saves the aggregated results to a CSV file.\"\"\"\n\n        df = self.to_dataframe()\n        if df.empty:\n            print(\"No results found to generate CSV.\")\n            return\n\n        if sort_by:\n            valid_sort_keys = [key for key in sort_by if key in df.columns]\n            if valid_sort_keys:\n                df = df.sort_values(by=valid_sort_keys).reset_index(drop=True)\n\n        df.to_csv(output_path, index=False)\n        print(f\"Results saved to {output_path}\")\n\n    def to_pivot_excel(self, output_path: str, df: pd.DataFrame, index_cols: list[str], column_cols: list[str],\n                       value_cols: list[str], add_ranking: bool = True):\n        \"\"\"Creates a pivot table from the results and saves it to an Excel file.\"\"\"\n\n        if df.empty:\n            print(\"DataFrame is empty, cannot generate pivot table.\")\n            return\n\n        try:\n            pivot_df = df.pivot_table(\n                index=index_cols,\n                columns=column_cols,\n                values=value_cols\n            )\n        except Exception as e:\n            print(f\"Could not create pivot table. Error: {e}\")\n            return\n\n        if not add_ranking:\n            pivot_df.to_excel(output_path)\n            print(f\"Pivot table saved to {output_path}\")\n            return\n\n        rank_df = pivot_df.rank(method='min', ascending=True)\n        final_pivot = pivot_df.astype(str)\n        rank_labels = {1: ' (1st)', 2: ' (2nd)', 3: ' (3rd)'}\n\n        for col in final_pivot.columns:\n            for idx in final_pivot.index:\n                value = pivot_df.at[idx, col]\n                if pd.notna(value):\n                    rank = rank_df.at[idx, col]\n                    rank_label = rank_labels.get(rank, '')\n                    final_pivot.at[idx, col] = f\"{value:.4f}{rank_label}\"\n                else:\n                    final_pivot.at[idx, col] = \"\"\n\n        final_pivot.to_excel(output_path)\n        print(f\"Pivot table with ranking saved to {output_path}\")\n\n    def clean_results(self,\n                      incomplete_marker: str = SUCCESS_MARKER,  # Default to the reliable marker\n                      filter_func: callable = None,\n                      dry_run: bool = True):\n        \"\"\"\n        Deletes result folders based on specified criteria.\n\n        Args:\n            incomplete_marker (str, optional): A filename (e.g., 'metrics.npy' or 'final.pth')\n                that marks an experiment as complete. Folders missing this file will be\n                targeted for deletion.\n            filter_func (callable, optional): A function that takes a configuration\n                dictionary (from args.json) and returns True if the folder should be deleted.\n            dry_run (bool): If True, only prints which folders would be deleted without\n                actually deleting them. Set to False to perform deletion.\n        \"\"\"\n        folders_to_delete = set()\n        all_folders = utils.get_subdirectories(str(self.results_path))\n\n        # 1. Identify folders based on criteria\n        for folder in all_folders:\n            # Criterion 1: Check for incomplete runs\n            if incomplete_marker and not (folder / incomplete_marker).exists():\n                folders_to_delete.add(folder)\n                print(f\"[INCOMPLETE] Marked for deletion: {folder.name} (missing '{incomplete_marker}')\")\n                continue  # Move to next folder once marked\n\n            # Criterion 2: Apply custom filter function\n            if filter_func:\n                args_path = folder / 'args.json'\n                if args_path.exists():\n                    config = utils.load_json(str(args_path))\n                    if config and filter_func(config):\n                        folders_to_delete.add(folder)\n                        print(f\"[FILTER MATCH] Marked for deletion: {folder.name}\")\n\n        if not folders_to_delete:\n            print(\"No folders matched the cleaning criteria.\")\n            return\n\n        print(\"\\n\" + \"=\" * 50)\n        print(f\"Found {len(folders_to_delete)} folders to delete.\")\n        print(\"=\" * 50)\n\n        # 2. Perform deletion (or simulate if dry_run)\n        if dry_run:\n            print(\"\\n[DRY RUN] The following folders would be deleted:\")\n            for folder in sorted(list(folders_to_delete)):\n                print(f\"  - {folder.name}\")\n            print(\"\\nTo delete these folders, run this method with `dry_run=False`.\")\n        else:\n            # Confirmation step for safety\n            confirm = input(\n                f\"Are you sure you want to permanently delete these {len(folders_to_delete)} folders? (yes/no): \")\n            if confirm.lower() == 'yes':\n                print(\"Deleting folders...\")\n                for folder in folders_to_delete:\n                    try:\n                        shutil.rmtree(folder)\n                        print(f\"  - Deleted: {folder.name}\")\n                    except Exception as e:\n                        print(f\"  - Error deleting {folder.name}: {e}\")\n                print(\"Cleaning complete.\")\n            else:\n                print(\"Deletion cancelled by user.\")\n</code></pre>"},{"location":"api/#ztxexp.analyzer.ResultAnalyzer.clean_results","title":"<code>clean_results(incomplete_marker=SUCCESS_MARKER, filter_func=None, dry_run=True)</code>","text":"<p>Deletes result folders based on specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>incomplete_marker</code> <code>str</code> <p>A filename (e.g., 'metrics.npy' or 'final.pth') that marks an experiment as complete. Folders missing this file will be targeted for deletion.</p> <code>SUCCESS_MARKER</code> <code>filter_func</code> <code>callable</code> <p>A function that takes a configuration dictionary (from args.json) and returns True if the folder should be deleted.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only prints which folders would be deleted without actually deleting them. Set to False to perform deletion.</p> <code>True</code> Source code in <code>ztxexp\\analyzer.py</code> <pre><code>def clean_results(self,\n                  incomplete_marker: str = SUCCESS_MARKER,  # Default to the reliable marker\n                  filter_func: callable = None,\n                  dry_run: bool = True):\n    \"\"\"\n    Deletes result folders based on specified criteria.\n\n    Args:\n        incomplete_marker (str, optional): A filename (e.g., 'metrics.npy' or 'final.pth')\n            that marks an experiment as complete. Folders missing this file will be\n            targeted for deletion.\n        filter_func (callable, optional): A function that takes a configuration\n            dictionary (from args.json) and returns True if the folder should be deleted.\n        dry_run (bool): If True, only prints which folders would be deleted without\n            actually deleting them. Set to False to perform deletion.\n    \"\"\"\n    folders_to_delete = set()\n    all_folders = utils.get_subdirectories(str(self.results_path))\n\n    # 1. Identify folders based on criteria\n    for folder in all_folders:\n        # Criterion 1: Check for incomplete runs\n        if incomplete_marker and not (folder / incomplete_marker).exists():\n            folders_to_delete.add(folder)\n            print(f\"[INCOMPLETE] Marked for deletion: {folder.name} (missing '{incomplete_marker}')\")\n            continue  # Move to next folder once marked\n\n        # Criterion 2: Apply custom filter function\n        if filter_func:\n            args_path = folder / 'args.json'\n            if args_path.exists():\n                config = utils.load_json(str(args_path))\n                if config and filter_func(config):\n                    folders_to_delete.add(folder)\n                    print(f\"[FILTER MATCH] Marked for deletion: {folder.name}\")\n\n    if not folders_to_delete:\n        print(\"No folders matched the cleaning criteria.\")\n        return\n\n    print(\"\\n\" + \"=\" * 50)\n    print(f\"Found {len(folders_to_delete)} folders to delete.\")\n    print(\"=\" * 50)\n\n    # 2. Perform deletion (or simulate if dry_run)\n    if dry_run:\n        print(\"\\n[DRY RUN] The following folders would be deleted:\")\n        for folder in sorted(list(folders_to_delete)):\n            print(f\"  - {folder.name}\")\n        print(\"\\nTo delete these folders, run this method with `dry_run=False`.\")\n    else:\n        # Confirmation step for safety\n        confirm = input(\n            f\"Are you sure you want to permanently delete these {len(folders_to_delete)} folders? (yes/no): \")\n        if confirm.lower() == 'yes':\n            print(\"Deleting folders...\")\n            for folder in folders_to_delete:\n                try:\n                    shutil.rmtree(folder)\n                    print(f\"  - Deleted: {folder.name}\")\n                except Exception as e:\n                    print(f\"  - Error deleting {folder.name}: {e}\")\n            print(\"Cleaning complete.\")\n        else:\n            print(\"Deletion cancelled by user.\")\n</code></pre>"},{"location":"api/#ztxexp.analyzer.ResultAnalyzer.to_csv","title":"<code>to_csv(output_path, sort_by=None)</code>","text":"<p>Saves the aggregated results to a CSV file.</p> Source code in <code>ztxexp\\analyzer.py</code> <pre><code>def to_csv(self, output_path: str, sort_by: list[str] = None):\n    \"\"\"Saves the aggregated results to a CSV file.\"\"\"\n\n    df = self.to_dataframe()\n    if df.empty:\n        print(\"No results found to generate CSV.\")\n        return\n\n    if sort_by:\n        valid_sort_keys = [key for key in sort_by if key in df.columns]\n        if valid_sort_keys:\n            df = df.sort_values(by=valid_sort_keys).reset_index(drop=True)\n\n    df.to_csv(output_path, index=False)\n    print(f\"Results saved to {output_path}\")\n</code></pre>"},{"location":"api/#ztxexp.analyzer.ResultAnalyzer.to_dataframe","title":"<code>to_dataframe(results_filename='results.json')</code>","text":"<p>Aggregates results into a pandas DataFrame. It now only considers folders with a _SUCCESS marker and merges 'args.json' with a specified results file.</p> <p>Parameters:</p> Name Type Description Default <code>results_filename</code> <code>str</code> <p>The name of the file containing experiment                     metrics (e.g., 'results.json').</p> <code>'results.json'</code> Source code in <code>ztxexp\\analyzer.py</code> <pre><code>def to_dataframe(self, results_filename: str = 'results.json') -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates results into a pandas DataFrame.\n    It now only considers folders with a _SUCCESS marker and merges\n    'args.json' with a specified results file.\n\n    Args:\n        results_filename (str): The name of the file containing experiment\n                                metrics (e.g., 'results.json').\n    \"\"\"\n    folders = utils.get_subdirectories(str(self.results_path))\n    records = []\n\n    print(f\"Analyzing results... Looking for '{SUCCESS_MARKER}' and '{results_filename}'.\")\n\n    for folder in folders:\n        # ONLY consider folders that have the success marker\n        if not (folder / SUCCESS_MARKER).exists():\n            continue\n\n        args_path = folder / 'args.json'\n        if args_path.exists():\n            record = utils.load_json(str(args_path))\n            if not record:\n                continue\n\n            # Load results file and merge it into the record\n            results_path = folder / results_filename\n            if results_path.exists():\n                results_data = utils.load_json(str(results_path))\n                if results_data:\n                    record.update(results_data)\n\n            record['setting_path'] = str(folder.resolve())\n            record['creation_time'] = utils.get_file_creation_time(args_path)\n            records.append(record)\n\n    if not records:\n        print(\"No successfully completed experiments found.\")\n        return pd.DataFrame()\n\n    return pd.DataFrame.from_records(records)\n</code></pre>"},{"location":"api/#ztxexp.analyzer.ResultAnalyzer.to_pivot_excel","title":"<code>to_pivot_excel(output_path, df, index_cols, column_cols, value_cols, add_ranking=True)</code>","text":"<p>Creates a pivot table from the results and saves it to an Excel file.</p> Source code in <code>ztxexp\\analyzer.py</code> <pre><code>def to_pivot_excel(self, output_path: str, df: pd.DataFrame, index_cols: list[str], column_cols: list[str],\n                   value_cols: list[str], add_ranking: bool = True):\n    \"\"\"Creates a pivot table from the results and saves it to an Excel file.\"\"\"\n\n    if df.empty:\n        print(\"DataFrame is empty, cannot generate pivot table.\")\n        return\n\n    try:\n        pivot_df = df.pivot_table(\n            index=index_cols,\n            columns=column_cols,\n            values=value_cols\n        )\n    except Exception as e:\n        print(f\"Could not create pivot table. Error: {e}\")\n        return\n\n    if not add_ranking:\n        pivot_df.to_excel(output_path)\n        print(f\"Pivot table saved to {output_path}\")\n        return\n\n    rank_df = pivot_df.rank(method='min', ascending=True)\n    final_pivot = pivot_df.astype(str)\n    rank_labels = {1: ' (1st)', 2: ' (2nd)', 3: ' (3rd)'}\n\n    for col in final_pivot.columns:\n        for idx in final_pivot.index:\n            value = pivot_df.at[idx, col]\n            if pd.notna(value):\n                rank = rank_df.at[idx, col]\n                rank_label = rank_labels.get(rank, '')\n                final_pivot.at[idx, col] = f\"{value:.4f}{rank_label}\"\n            else:\n                final_pivot.at[idx, col] = \"\"\n\n    final_pivot.to_excel(output_path)\n    print(f\"Pivot table with ranking saved to {output_path}\")\n</code></pre>"},{"location":"api/#ztxexp.utils--general-utilities","title":"General Utilities","text":"<p>This module provides general utility functions used across the framework, including path management, logging, serialization, timing, hashing, and display formatting.</p> <p>Functions:</p> Name Description <code>add_to_sys_path</code> <p>Adds a directory to the Python system path.</p> <code>setup_logger</code> <p>Sets up a logger that writes to both a file and the console.</p> <code>save_json</code> <p>Saves a dictionary to a JSON file.</p> <code>load_json</code> <p>Loads a dictionary from a JSON file.</p> <code>save_dill</code> <p>Serializes and saves a Python object using dill.</p> <code>load_dill</code> <p>Loads and deserializes a Python object using dill.</p> <code>save_torch_model</code> <p>Saves a PyTorch model checkpoint.</p> <code>load_torch_model</code> <p>Loads a PyTorch model checkpoint.</p> <code>timer</code> <p>A context manager to time a block of code.</p> <code>format_time_delta</code> <p>Formats a duration in seconds into a human-readable string.</p> <code>get_memory_usage</code> <p>Returns the memory usage of the current process in MB.</p> <code>config_to_hash</code> <p>Creates a deterministic hash from a configuration dictionary.</p> <code>pretty_print_namespace</code> <p>Prints argparse.Namespace in a formatted and colorful way.</p> <code>pretty_print_dict</code> <p>Prints a dictionary in a formatted and colorful way.</p>"},{"location":"api/#ztxexp.utils.add_to_sys_path","title":"<code>add_to_sys_path(path)</code>","text":"<p>Adds a directory to the Python system path if it's not already there.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory to be added.</p> required Source code in <code>ztxexp\\utils.py</code> <pre><code>def add_to_sys_path(path: str):\n    \"\"\"\n    Adds a directory to the Python system path if it's not already there.\n\n    Args:\n        path (str): The path to the directory to be added.\n    \"\"\"\n    abs_path = str(Path(path).resolve())\n    if abs_path not in sys.path:\n        sys.path.insert(0, abs_path)\n        print(f\"Added '{abs_path}' to system path.\")\n</code></pre>"},{"location":"api/#ztxexp.utils.config_to_hash","title":"<code>config_to_hash(config, length=8)</code>","text":"<p>Creates a deterministic hash from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> required <code>length</code> <code>int</code> <p>The desired length of the hash string.</p> <code>8</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A truncated SHA256 hash.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def config_to_hash(config: dict, length: int = 8) -&gt; str:\n    \"\"\"\n    Creates a deterministic hash from a configuration dictionary.\n\n    Args:\n        config (dict): The configuration dictionary.\n        length (int): The desired length of the hash string.\n\n    Returns:\n        str: A truncated SHA256 hash.\n    \"\"\"\n    # Sort the dictionary to ensure order doesn't affect the hash\n    sorted_config_str = json.dumps(config, sort_keys=True)\n    hash_object = hashlib.sha256(sorted_config_str.encode('utf-8'))\n    return hash_object.hexdigest()[:length]\n</code></pre>"},{"location":"api/#ztxexp.utils.create_dir","title":"<code>create_dir(path)</code>","text":"<p>Create a directory if it does not exist.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def create_dir(path: str):\n    \"\"\"\n    Create a directory if it does not exist.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n</code></pre>"},{"location":"api/#ztxexp.utils.delete_dir","title":"<code>delete_dir(path)</code>","text":"<p>Delete a directory and all its contents.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def delete_dir(path: str):\n    \"\"\"\n    Delete a directory and all its contents.\n    \"\"\"\n    if os.path.exists(path) and os.path.isdir(path):\n        shutil.rmtree(path)\n        print(f'Deleted directory: {path}')\n</code></pre>"},{"location":"api/#ztxexp.utils.format_time_delta","title":"<code>format_time_delta(seconds)</code>","text":"<p>Formats a duration in seconds into a human-readable string (H, M, S).</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def format_time_delta(seconds: float) -&gt; str:\n    \"\"\"Formats a duration in seconds into a human-readable string (H, M, S).\"\"\"\n    h = int(seconds // 3600)\n    m = int((seconds % 3600) // 60)\n    s = int(seconds % 60)\n    return f\"{h}h {m}m {s}s\"\n</code></pre>"},{"location":"api/#ztxexp.utils.get_file_creation_time","title":"<code>get_file_creation_time(file_path)</code>","text":"<p>Get the creation time of a file formatted as a string.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def get_file_creation_time(file_path: str) -&gt; str:\n    \"\"\"\n    Get the creation time of a file formatted as a string.\n    \"\"\"\n    path = pathlib.Path(file_path)\n    timestamp = path.stat().st_ctime\n    creation_time = datetime.fromtimestamp(timestamp)\n    return creation_time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n</code></pre>"},{"location":"api/#ztxexp.utils.get_memory_usage","title":"<code>get_memory_usage()</code>","text":"<p>Returns the memory usage of the current process in MB.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def get_memory_usage() -&gt; str:\n    \"\"\"Returns the memory usage of the current process in MB.\"\"\"\n    process = psutil.Process(os.getpid())\n    mem_info = process.memory_info()\n    return f\"{mem_info.rss / 1024 ** 2:.2f} MB\"\n</code></pre>"},{"location":"api/#ztxexp.utils.get_subdirectories","title":"<code>get_subdirectories(path)</code>","text":"<p>Get all subdirectories in a given path.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def get_subdirectories(path: str) -&gt; list[pathlib.Path]:\n    \"\"\"\n    Get all subdirectories in a given path.\n    \"\"\"\n    p = pathlib.Path(path)\n    if not p.exists() or not p.is_dir():\n        return []\n    return [folder for folder in p.iterdir() if folder.is_dir()]\n</code></pre>"},{"location":"api/#ztxexp.utils.load_dill","title":"<code>load_dill(file_path)</code>","text":"<p>Loads and deserializes a Python object using dill.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def load_dill(file_path: str) -&gt; object:\n    \"\"\"Loads and deserializes a Python object using dill.\"\"\"\n    with open(file_path, 'rb') as f:\n        return dill.load(f)\n</code></pre>"},{"location":"api/#ztxexp.utils.load_json","title":"<code>load_json(file_path)</code>","text":"<p>Loads a dictionary from a JSON file.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def load_json(file_path: str) -&gt; dict | None:\n    \"\"\"Loads a dictionary from a JSON file.\"\"\"\n    if not os.path.exists(file_path):\n        return None\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/#ztxexp.utils.load_torch_model","title":"<code>load_torch_model(model, optimizer, path)</code>","text":"<p>Loads a PyTorch model checkpoint.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def load_torch_model(model: torch.nn.Module, optimizer: torch.optim.Optimizer, path: str) -&gt; tuple:\n    \"\"\"Loads a PyTorch model checkpoint.\"\"\"\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    return model, optimizer, epoch\n</code></pre>"},{"location":"api/#ztxexp.utils.pretty_print_dict","title":"<code>pretty_print_dict(d, items_per_line=3)</code>","text":"<p>Prints a dictionary in a formatted and colorful way.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def pretty_print_dict(d: dict, items_per_line: int = 3):\n    \"\"\"Prints a dictionary in a formatted and colorful way.\"\"\"\n    if not d:\n        print(\"No items in dictionary to print.\")\n        return\n\n    key_width = max([len(k) for k in d.keys()]) + 4\n    val_width = max([len(str(v)) for v in d.values()]) + 4\n\n    items = sorted(d.items(), key=lambda x: x[0])\n\n    for i in range(0, len(items), items_per_line):\n        line = \"\"\n        for j in range(items_per_line):\n            if i + j &lt; len(items):\n                key, value = items[i + j]\n                line += f\"| \\033[92m {key:&lt;{key_width}} \\033[94m{str(value):&gt;{val_width}} \\033[0m\"\n        line += \"|\"\n        print(line)\n</code></pre>"},{"location":"api/#ztxexp.utils.pretty_print_namespace","title":"<code>pretty_print_namespace(args, items_per_line=3)</code>","text":"<p>Prints argparse.Namespace in a formatted and colorful way.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def pretty_print_namespace(args, items_per_line: int = 3):\n    \"\"\"Prints argparse.Namespace in a formatted and colorful way.\"\"\"\n    args_dict = vars(args)\n    if not args_dict:\n        print(\"No arguments to print.\")\n        return\n\n    pretty_print_dict(args_dict, items_per_line)\n</code></pre>"},{"location":"api/#ztxexp.utils.save_dill","title":"<code>save_dill(obj, file_path)</code>","text":"<p>Serializes and saves a Python object using dill.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def save_dill(obj: object, file_path: str):\n    \"\"\"Serializes and saves a Python object using dill.\"\"\"\n    with open(file_path, 'wb') as f:\n        dill.dump(obj, f)\n</code></pre>"},{"location":"api/#ztxexp.utils.save_json","title":"<code>save_json(data, file_path, indent=4)</code>","text":"<p>Saves a dictionary to a JSON file.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def save_json(data: dict, file_path: str, indent: int = 4):\n    \"\"\"Saves a dictionary to a JSON file.\"\"\"\n    with open(file_path, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False, indent=indent)\n</code></pre>"},{"location":"api/#ztxexp.utils.save_torch_model","title":"<code>save_torch_model(model, optimizer, epoch, path)</code>","text":"<p>Saves a PyTorch model checkpoint.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def save_torch_model(model: torch.nn.Module, optimizer: torch.optim.Optimizer, epoch: int, path: str):\n    \"\"\"Saves a PyTorch model checkpoint.\"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n    }, path)\n</code></pre>"},{"location":"api/#ztxexp.utils.setup_logger","title":"<code>setup_logger(name, log_file, level=logging.INFO)</code>","text":"<p>Sets up a logger that writes to both a file and the console.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the logger.</p> required <code>log_file</code> <code>str</code> <p>The path to the log file.</p> required <code>level</code> <p>The logging level (e.g., logging.INFO, logging.DEBUG).</p> <code>INFO</code> <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: The configured logger instance.</p> Source code in <code>ztxexp\\utils.py</code> <pre><code>def setup_logger(name: str, log_file: str, level=logging.INFO) -&gt; logging.Logger:\n    \"\"\"\n    Sets up a logger that writes to both a file and the console.\n\n    Args:\n        name (str): The name of the logger.\n        log_file (str): The path to the log file.\n        level: The logging level (e.g., logging.INFO, logging.DEBUG).\n\n    Returns:\n        logging.Logger: The configured logger instance.\n    \"\"\"\n    # Ensure log directory exists\n    log_dir = Path(log_file).parent\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # File handler\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setFormatter(formatter)\n\n    # Console handler\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    # Avoid adding handlers multiple times\n    if not logger.handlers:\n        logger.addHandler(file_handler)\n        logger.addHandler(stream_handler)\n\n    return logger\n</code></pre>"},{"location":"api/#ztxexp.utils.timer","title":"<code>timer(name, logger=None)</code>","text":"<p>A context manager to time a block of code.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the timed block.</p> required <code>logger</code> <code>Logger</code> <p>If provided, logs the time.</p> <code>None</code> Source code in <code>ztxexp\\utils.py</code> <pre><code>@contextmanager\ndef timer(name: str, logger: logging.Logger = None):\n    \"\"\"\n    A context manager to time a block of code.\n\n    Args:\n        name (str): The name of the timed block.\n        logger (logging.Logger, optional): If provided, logs the time.\n    \"\"\"\n    t0 = time.time()\n    yield\n    elapsed = time.time() - t0\n    message = f\"[{name}] done in {elapsed:.4f} s\"\n    if logger:\n        logger.info(message)\n    else:\n        print(message)\n</code></pre>"},{"location":"api/#ztxexp.environment--experiment-environment-setup","title":"Experiment Environment Setup","text":"<p>This module provides utilities for setting up the experiment environment, including GPU initialization, random seed setting, and process priority management.</p> <p>Functions:</p> Name Description <code>init_torch_env</code> <p>Initializes the PyTorch environment with specified parameters.</p> <code>set_process_priority</code> <p>Sets the priority of the current process.</p>"},{"location":"api/#ztxexp.environment.init_torch_env","title":"<code>init_torch_env(seed=3407, use_gpu=True, gpu_id=0, deterministic=False, benchmark=False)</code>","text":"<p>Initializes the environment for reproducible deep learning experiments.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The random seed.</p> <code>3407</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU if available.</p> <code>True</code> <code>gpu_id</code> <code>int</code> <p>The GPU ID to use.</p> <code>0</code> <code>deterministic</code> <code>bool</code> <p>Whether to use deterministic algorithms.</p> <code>False</code> <code>benchmark</code> <code>bool</code> <p>Whether to let cuDNN find the best algorithm.</p> <code>False</code> Source code in <code>ztxexp\\environment.py</code> <pre><code>def init_torch_env(\n        seed: int = 3407,\n        use_gpu: bool = True,\n        gpu_id: int = 0,\n        deterministic: bool = False,\n        benchmark: bool = False\n):\n    \"\"\"\n    Initializes the environment for reproducible deep learning experiments.\n\n    Args:\n        seed (int): The random seed.\n        use_gpu (bool): Whether to use GPU if available.\n        gpu_id (int): The GPU ID to use.\n        deterministic (bool): Whether to use deterministic algorithms.\n        benchmark (bool): Whether to let cuDNN find the best algorithm.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    if torch.cuda.is_available() and use_gpu:\n        torch.cuda.manual_seed_all(seed)\n        torch.cuda.set_device(gpu_id)\n        torch.backends.cudnn.deterministic = deterministic\n        torch.backends.cudnn.benchmark = benchmark\n        print(f\"Using GPU: {gpu_id}\")\n        return torch.device(f\"cuda:{gpu_id}\")\n    else:\n        print(\"Using CPU\")\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/#ztxexp.environment.set_process_priority","title":"<code>set_process_priority(priority='high')</code>","text":"<p>Set the priority of the current process. 'high', 'normal', 'low'</p> Source code in <code>ztxexp\\environment.py</code> <pre><code>def set_process_priority(priority: str = 'high'):\n    \"\"\"\n    Set the priority of the current process.\n    'high', 'normal', 'low'\n    \"\"\"\n    p = psutil.Process(os.getpid())\n    try:\n        if priority == 'high':\n            p.nice(psutil.HIGH_PRIORITY_CLASS)\n        elif priority == 'low':\n            p.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)\n        else:\n            p.nice(psutil.NORMAL_PRIORITY_CLASS)\n    except AttributeError:\n        # For non-Windows systems\n        # Lower nice value means higher priority\n        if priority == 'high':\n            p.nice(-10)\n        elif priority == 'low':\n            p.nice(10)\n        else:\n            p.nice(0)\n</code></pre>"}]}